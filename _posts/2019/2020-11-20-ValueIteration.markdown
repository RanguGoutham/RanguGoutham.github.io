---
layout: "post"
title: "ValueIteration"

excerpt: "MDPS"
comments: true
---

Value Function:\
- It represents how good is a state for the agent to be in, which is determined by the total 
discounted expected reward gained by the agent starting from state s and acting under policy $\pi$
- So the value function depends on the policy of the agent,

Valueiteration:\

Value:
- Expected discounted sum of rewards if acting OPTIMALLY from state S for i steps
- Expected discounted sum of rewards accumulated starting from state S, acting optimally for i steps.\

Policy:
- Optimal action when in state s and getting to act for i steps. 
    